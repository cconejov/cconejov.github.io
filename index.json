[{"authors":["admin"],"categories":null,"content":"Cesar Conejo Villalobos is an actuary graduated from the University of Costa Rica with five years of experience in bank and finance industry. Along this path, he has learned and applied different techniques in backend and frontend data science, especially in data storage infrastructure, efficient computing, data analysis, and prediction algorithms focused on fraud detection. Cesar is also a philosophy lover, a passion that he shares with data science through the study of data ethics and data democratization\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cconejov.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Cesar Conejo Villalobos is an actuary graduated from the University of Costa Rica with five years of experience in bank and finance industry. Along this path, he has learned and applied different techniques in backend and frontend data science, especially in data storage infrastructure, efficient computing, data analysis, and prediction algorithms focused on fraud detection.","tags":null,"title":"Cesar Conejo Villalobos","type":"authors"},{"authors":[],"categories":[],"content":"","date":1583725462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583725462,"objectID":"62d9c91182f69cacf45310931b26c720","permalink":"https://cconejov.github.io/project/unsupervised/","publishdate":"2020-03-08T21:44:22-06:00","relpermalink":"/project/unsupervised/","section":"project","summary":"","tags":[],"title":"Unsupervised","type":"project"},{"authors":null,"categories":null,"content":"Machine learning: Supervised learning. Currently, lots of people talk about machine learning and its applications. People consider that the success of companies rests in the opportunity of applying ML techniques in your business. In this way, it is important having some knowledge about these techniques and more importantly how to use these techniques.\nThe idea of this project is to show some of the techniques available in machine learning, and we are going to start with one of the big branches: supervised learning methodologies. Supervised learning scenario occurs when \u0026ldquo;a set of variables that might be denoted as inputs have some influence on one or more outputs\u0026rdquo; (Hastie, Tibshirani, and Friedman, 2009). The purpose of supervised learning is to use the inputs to predict the values of the outputs.\nOn the other hand, depending on the characteristic of the output, we have different problems. If the output is a quantitative variable, we have a regression problem. In case that the output is qualitative, we have a classification problem. However, in the case of some qualitative outcomes with two classes, we can use regression techniques into classification problems.\nClassification The focus of the section is to consider the classification problem, then compare some techniques in one simple exercise and finally to use a cross-validation method for estimating the prediction error. All these steps are made in R program using the following libraries:\nlibrary(tidyverse) #ggplot\rlibrary(kknn) #kknn\rlibrary(e1071) #svm\rlibrary(rpart) #tree\rlibrary(randomForest) #RF\rlibrary(ada) #Boost\rlibrary(caret) #CV\r Using the purchasedBikes file (see https://github.com/cconejov/purchased_Bike/tree/master/Data), this table contains 1000 observations of 12 variables. We want to use 11 predictor variables for predict the binary class as output PurchasedBike. It indicates if a customer bought or not a bicycle.\npurchased_bike \u0026lt;- read.table(\u0026quot;purchasedBikes.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;;\u0026quot;,\rdec = \u0026quot;,\u0026quot;,\rrow.names = 1)\r Because the purpose of this exercise is to compare the performance of some techniques, we can see that the outcome variable PurchasedBike is well balanced (48% of the observations in the data set corresponding to Yes value).\n\r\rA caption\r\r\rggplot(data = purchased_bike) +\rgeom_bar(aes( x = PurchasedBike, fill = PurchasedBike)) +\rlabs(title = \u0026quot;Distribution Purchased Bike\u0026quot;)  The goal of this exercise is to compare several classification techniques with this example table. Here, the purpose is to predict Yes value in the PurchasedBike using ten cross-validations with 5 groups with the methods:\n Support Vector Machine KNN Bayes Decision tree Random Forest Boosting methods (AdaBoost)  Furthermore, the two metrics that we are going to use for determining how good our model is are:\n How many \u0026ldquo;yes\u0026rdquo; purchase bike variable the model detects. High values are best. Minimum average global error, calculated as 1 - accuracy, where accuracy is calculated as the number of correct predictions divided by the number of total predictions. Low values are best.  For determining the number of \u0026ldquo;yes\u0026rdquo; in the PurchasedBike and the average global error, we do the following code:\n#Number of observation\rn \u0026lt;- dim(purchased_bike)[1]\r#SVM\ryes_detect_svm \u0026lt;- rep(0,10)\rerror_detect_svm \u0026lt;- rep(0,10)\r#KNN\ryes_detect_knn \u0026lt;- rep(0,10)\rerror_detect_knn \u0026lt;- rep(0,10)\r#Bayes\ryes_detect_bayes \u0026lt;- rep(0,10)\rerror_detect_bayes \u0026lt;- rep(0,10)\r#trees\ryes_detect_tree \u0026lt;- rep(0,10)\rerror_detect_tree \u0026lt;- rep(0,10)\r#RFs\ryes_detect_RF \u0026lt;- rep(0,10)\rerror_detect_RF \u0026lt;- rep(0,10)\r#boosting\ryes_detect_boosting \u0026lt;- rep(0,10)\rerror_detect_boosting \u0026lt;- rep(0,10)\r# cross validation 10 times\rfor(i in 1:10) { #10\rgroups \u0026lt;- createFolds(1:n,5) #5 groups\r#SVM\ryes_svm \u0026lt;- 0\rerror_svm \u0026lt;- 0\r#KNN\ryes_knn \u0026lt;- 0\rerror_knn \u0026lt;- 0\r#Bayes\ryes_bayes \u0026lt;- 0\rerror_bayes \u0026lt;- 0\r#trees\ryes_tree \u0026lt;- 0\rerror_tree \u0026lt;- 0\r#RFs\ryes_RF \u0026lt;- 0\rerror_RF \u0026lt;- 0\r#boosting\ryes_boosting \u0026lt;- 0\rerror_boosting \u0026lt;- 0\r# This for does \u0026quot;cross-validation\u0026quot; with 5 groups (Folds)\rfor(k in 1:5) { #5\rtraining \u0026lt;- groups[[k]] #list\rttesting \u0026lt;- purchased_bike[training,]\rtlearning \u0026lt;- purchased_bike[-training,]\r#SVM\rmodel \u0026lt;- svm(PurchasedBike~., data = tlearning, kernel =\u0026quot;radial\u0026quot;)\rprediction \u0026lt;- predict(model, ttesting)\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_svm \u0026lt;- yes_svm + MC[2,2]\r# Error detection\rerror_svm \u0026lt;- error_svm + (1 - (sum(diag(MC)))/sum(MC))*100\r#KNN\rmodel \u0026lt;- train.kknn(PurchasedBike~., data = tlearning, kmax = 7)\rprediction \u0026lt;- predict(model,ttesting[, -12])\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_knn \u0026lt;- yes_knn + MC[2,2]\r# Error detection\rerror_knn \u0026lt;- error_knn + (1 - (sum(diag(MC)))/sum(MC))*100\r#Bayes\rmodel \u0026lt;- naiveBayes(PurchasedBike~., data = tlearning)\rprediction \u0026lt;- predict(model, ttesting[,-12])\rActual \u0026lt;- ttesting[,12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_bayes \u0026lt;- yes_bayes + MC[2,2]\r# Error detection\rerror_bayes \u0026lt;- error_bayes + (1 - (sum(diag(MC)))/sum(MC))*100\r#trees\rmodel \u0026lt;- rpart(PurchasedBike~. ,data = tlearning)\rprediction \u0026lt;- predict(model, ttesting, type='class')\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_tree \u0026lt;- yes_tree + MC[2,2]\r# Error detection\rerror_tree \u0026lt;- error_tree + (1 - (sum(diag(MC)))/sum(MC))*100\r#RFs\rmodel \u0026lt;- randomForest(PurchasedBike~., data = tlearning, importance=TRUE)\rprediction \u0026lt;- predict(model, ttesting[, -12])\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_RF \u0026lt;- yes_RF + MC[2,2]\r# Error detection\rerror_RF \u0026lt;- error_RF + (1 - (sum(diag(MC)))/sum(MC))*100\r#boosting\rmodel \u0026lt;- ada(PurchasedBike~., data = tlearning, iter=20, nu = 1, type = \u0026quot;discrete\u0026quot;)\rprediction \u0026lt;- predict(model, ttesting[, -12])\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_boosting \u0026lt;- yes_boosting + MC[2,2]\r# Error detection\rerror_boosting \u0026lt;- error_boosting + (1-(sum(diag(MC)))/sum(MC))*100\r}\r#SVM yes_detect_svm[i] \u0026lt;- yes_svm\rerror_detect_svm[i] \u0026lt;- error_svm/5\r#KNN\ryes_detect_knn[i] \u0026lt;- yes_knn\rerror_detect_knn[i] \u0026lt;- error_knn/5\r#Bayes\ryes_detect_bayes[i] \u0026lt;- yes_bayes\rerror_detect_bayes[i] \u0026lt;- error_bayes/5\r#trees\ryes_detect_tree[i] \u0026lt;- yes_tree\rerror_detect_tree[i] \u0026lt;- error_tree/5\r#RF\ryes_detect_RF[i] \u0026lt;- yes_RF\rerror_detect_RF[i] \u0026lt;- error_RF/5\r#boosting\ryes_detect_boosting[i] \u0026lt;- yes_boosting\rerror_detect_boosting[i] \u0026lt;- error_boosting/5\r}\r Finally, we create three auxiliary tables for showing the results:\ncolors \u0026lt;- c(\u0026quot;SVM\u0026quot; = \u0026quot;blue\u0026quot;,\r\u0026quot;KNN\u0026quot; = \u0026quot;red\u0026quot;,\r\u0026quot;Bayes\u0026quot; = \u0026quot;orange\u0026quot;,\r\u0026quot;Tree\u0026quot; = \u0026quot;purple\u0026quot;,\r\u0026quot;RF\u0026quot; = \u0026quot;black\u0026quot;,\r\u0026quot;Boost\u0026quot; = \u0026quot;green\u0026quot;\r)\rdetect \u0026lt;- tibble(Iteration = seq(1,10),\rSVM = yes_detect_svm,\rKNN = yes_detect_knn,\rBayes = yes_detect_bayes,\rTree = yes_detect_tree,\rRF = yes_detect_RF,\rBoost = yes_detect_boosting)\rglobal_error \u0026lt;- tibble(Iteration = seq(1,10),\rSVM = error_detect_svm,\rKNN = error_detect_knn,\rBayes = error_detect_bayes,\rTree = error_detect_tree,\rRF = error_detect_RF,\rBoost = error_detect_boosting)\r The plot with the yes value detection shows us that the method random forest has on average the best capacity for detecting the positive cases of bikes purchased.\nggplot(data = detect, aes(x = Iteration) ) +\rgeom_line(aes( y = SVM, color = \u0026quot;SVM\u0026quot;) ) +\rgeom_line(aes( y = KNN, color = \u0026quot;KNN\u0026quot;)) +\rgeom_line(aes( y = Bayes, color = \u0026quot;Bayes\u0026quot;)) +\rgeom_line(aes( y = Tree, color = \u0026quot;Tree\u0026quot;)) +\rgeom_line(aes( y = RF, color = \u0026quot;RF\u0026quot;), size = 1) +\rgeom_line(aes( y = Boost, color = \u0026quot;Boost\u0026quot;)) +\rscale_x_continuous(breaks=c(1:10), labels=c(1:10),limits=c(1,10)) +\rlabs(x = \u0026quot;Iteration\u0026quot;,\ry = \u0026quot;Yes detection\u0026quot;,\rcolor = \u0026quot;Method\u0026quot;,\rtitle = \u0026quot;Quantity of Purchased bike variable\u0026quot;\r) +\rscale_color_manual(values = colors)\r \r\rA caption\r\r\rBesides, Random forest also has the lest average global error, so this method also classifies very well the negative case of bike did not purchase.\nggplot(data = global_error, aes(x = Iteration) ) +\rgeom_line(aes( y = SVM, color = \u0026quot;SVM\u0026quot;) ) +\rgeom_line(aes( y = KNN, color = \u0026quot;KNN\u0026quot;)) +\rgeom_line(aes( y = Bayes, color = \u0026quot;Bayes\u0026quot;)) +\rgeom_line(aes( y = Tree, color = \u0026quot;Tree\u0026quot;)) +\rgeom_line(aes( y = RF, color = \u0026quot;RF\u0026quot;), size = 1) +\rgeom_line(aes( y = Boost, color = \u0026quot;Boost\u0026quot;)) +\rscale_x_continuous(breaks=c(1:10), labels=c(1:10),limits=c(1,10)) +\rlabs(x = \u0026quot;Iteration\u0026quot;,\ry = \u0026quot;AVG global Error\u0026quot;,\rcolor = \u0026quot;Method\u0026quot;,\rtitle = \u0026quot;Average global error by iteration\u0026quot;\r) +\rscale_color_manual(values = colors)\r \r\rA caption\r\r\r","date":1583539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583539200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://cconejov.github.io/project/internal-project/","publishdate":"2020-03-07T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An introduction to supervised machine learning algorithms, especially methods for classification problems.","tags":["Machine Learning","Supervised Algorithms","Classification"],"title":"Supervised Algorithms","type":"project"}]